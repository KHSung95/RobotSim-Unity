# 📅 SimuGuide 프로젝트 마일스톤 (최종 데드라인: 1월 9일)

AI 에이전트와 함께하는 1인 개발 기준, 1월 9일 마감 및 고퀄리티 데모 완성을 위한 세부 계획입니다.

---

## 🏁 Phase 1: 시스템 안정화 및 UI 연동 (12/23 - 12/29)
**목표**: 모든 컴파일 에러를 제거하고, UI 버튼으로 로봇과 카메라를 제어할 수 있는 상태를 구축.

- [x] **에러 수정**: `PointCloudGenerator`, `GuidanceManager`의 변수 선언 누락 및 참조 에러 해결.
- [x] **센서 고도화**: 가상 카메라에 '산업용 노이즈' 모델 추가 (실제 정합의 불확실성 모사).
- [ ] **UI 바인딩**: 사이드바의 FK/IK 조그 버튼을 실제 로봇 관절 및 MoveIt2 명령과 연결.
- [ ] **카메라 마운트**: Hand-Eye vs Bird-Eye 모드 전환 및 오프셋 수동 설정 기능 검증.
- [ ] **검증**: 유니티 내에서 로봇을 조작하고, 카메라 피드를 보며 '마스터 캡처'가 가능한지 확인.

## 🔄 Phase 2: 가이던스 풀 루프 및 ROS2 연동 (12/30 - 1/05)
**목표**: "스캔 -> 보정값 도출 -> 이동"으로 이어지는 공정 시퀀스 완성.

- [ ] **좌표계 체인 완성**: $T_{ic}$ 공식을 적용하여 문짝이 어느 위치에 있든 정확한 보정 행렬($T_{corr}$) 도출.
- [ ] **ROS2 이동 연동**: 도출된 $T_{corr}$을 ROS2 `/target_pose`로 전송하여 MoveIt2가 로봇을 보정 위치로 이동시키도록 구축.
- [ ] **시퀀스 자동화**: 1-Grip(촬영), 2-Install(이동 후 재확인) 단계별 상태 머신 로직 구현.
- [ ] **히트맵 시각화**: 보정 전(Red)과 보정 후(Green)의 포인트클라우드 중첩 시뮬레이션 성공.

## 💎 Phase 3: 최종 폴리싱 및 발표 자료 준비 (1/06 - 1/09)
**목표**: 현업 공정 전문가를 설득할 수 있는 수준의 데모 영상 및 문서 완성.

- [ ] **UX 최적화**: 멀티 뷰포트(RGB/Depth), 로그 콘솔, 조그 속도 조절 등 디테일 요소 보완.
- [ ] **데모 영상 촬영**: 문짝 틀어짐 보정 공정 전체 과정을 녹화 (비전 가이던스의 정교함 강조).
- [ ] **발표 문서 최종화**: 'UX 기반 혁신' 및 '산업 표준 로직($T_{ic}$)'을 강조한 최종 PPT/README 업데이트.
- [ ] **최종 버그 픽스**: 비정상적인 위치 이동 시 예외 처리 등 시스템 안정성 확보.

---

### 💡 전략적 가이드 (1인 개발 최적화)
1. **눈속임이 아닌 '수학적 증명'**: 포인트클라우드 생성은 가상 센서(PCG)를 쓰되, 정합 결과는 유니티 좌표값 차이를 활용하여 100% 신뢰할 수 있는 결과 행렬을 먼저 만듭니다. (알고리즘 구현 시간 절약)
2. **AI 에이전트 적극 활용**: 복잡한 쿼터니언 변환이나 UI 이벤트 처리는 제가 작성하는 코드를 신뢰하고, 사용자님은 **'씬 구성(Scene Setup)'**과 **'시나리오 검증'**에 집중해 주세요.
3. **핵심 우선주의**: 시간이 부족할 경우 로봇 모델 추가나 화려한 애니메이션 대신, **"정합 결과 Matrix가 얼마나 정확히 도출되는가"**를 보여주는 데 집중합니다.
